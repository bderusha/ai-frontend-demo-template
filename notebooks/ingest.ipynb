{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse the Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "\n",
    "# Load the environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "FORM_RECOGNIZER_ENDPOINT = os.getenv(\"AZURE_FORM_RECOGNIZER_ENDPOINT\")\n",
    "FORM_RECOGNIZER_KEY = os.getenv(\"AZURE_FORM_RECOGNIZER_KEY\")\n",
    "\n",
    "if not FORM_RECOGNIZER_ENDPOINT or not FORM_RECOGNIZER_KEY:\n",
    "    raise ValueError(\n",
    "        f\"Please provide FORM_RECOGNIZER_ENDPOINT ({'set' if FORM_RECOGNIZER_ENDPOINT else 'None'}) and FORM_RECOGNIZER_KEY ({'set' if FORM_RECOGNIZER_KEY else 'None'}) as environment variables.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "docint_client = DocumentIntelligenceClient(\n",
    "    endpoint=FORM_RECOGNIZER_ENDPOINT, credential=AzureKeyCredential(FORM_RECOGNIZER_KEY),\n",
    ")\n",
    "\n",
    "pdfs_dir = Path(\"../public/pdfs\")\n",
    "pickled_pdfs_dir = Path('./data')\n",
    "pickled_pdfs_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the documents and pickle the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.documentintelligence.models import ContentFormat\n",
    "\n",
    "for file_name in pdfs_dir.iterdir():\n",
    "    print('Analyzing:', file_name.stem)\n",
    "    with open(f'{file_name}', 'rb') as doc:\n",
    "        handler = docint_client.begin_analyze_document(\n",
    "            model_id=\"prebuilt-layout\",\n",
    "            analyze_request=doc,\n",
    "            content_type=\"application/octet-stream\",\n",
    "            output_content_format=ContentFormat.MARKDOWN,\n",
    "        )\n",
    "\n",
    "    parsed_doc = handler.result()\n",
    "    \n",
    "    # Pickle the value and save it to the file\n",
    "    output_file_path = pickled_pdfs_dir / f'{file_name.stem}_markdown.pkl'\n",
    "    with output_file_path.open('wb') as output_file:\n",
    "        pickle.dump(parsed_doc, output_file)\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the pickled results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the parsed_documents dictionary from the pickled file\n",
    "\n",
    "parsed_pdfs = {}\n",
    "for file_path in pickled_pdfs_dir.iterdir():\n",
    "    with file_path.open('rb') as file:\n",
    "        parsed_pdfs[file_path.stem] = pickle.load(file)\n",
    "\n",
    "print('Parsed pdfs:\\n ', \"\\n  \".join(parsed_pdfs.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    AzureOpenAIVectorizer,\n",
    "    AzureOpenAIVectorizerParameters,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    SearchableField,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SearchIndex,\n",
    "    SemanticConfiguration,\n",
    "    SemanticField,\n",
    "    SemanticPrioritizedFields,\n",
    "    SemanticSearch,\n",
    "    SimpleField,\n",
    "    VectorSearch,\n",
    "    VectorSearchProfile,\n",
    ")\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "endpoint = os.environ[\"AZURE_SEARCH_SERVICE_ENDPOINT\"]\n",
    "credential = AzureKeyCredential(os.environ[\"AZURE_SEARCH_API_KEY\"])\n",
    "azure_openai_endpoint = os.environ[\"AZURE_OPENAI_API_ENDPOINT\"]\n",
    "azure_openai_key = os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    "azure_openai_embedding_deployment = os.getenv(\n",
    "    \"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\", \"text-embedding-3-large\"\n",
    ")\n",
    "azure_openai_embedding_dimensions = 1536\n",
    "embedding_model_name = os.getenv(\n",
    "    \"AZURE_OPENAI_EMBEDDING_MODEL\", \"text-embedding-3-large\"\n",
    ")\n",
    "azure_openai_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-10-21\")\n",
    "\n",
    "\n",
    "INDEX_NAME = \"index001\"\n",
    "\n",
    "index_client = SearchIndexClient(endpoint=endpoint, credential=credential)\n",
    "search_client = SearchClient(\n",
    "    endpoint=endpoint,\n",
    "    index_name=INDEX_NAME,\n",
    "    credential=credential,\n",
    ")\n",
    "aoai_client = AzureOpenAI(\n",
    "    api_key=azure_openai_key,\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    api_version=azure_openai_api_version,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\n",
    "    SimpleField(\n",
    "        name=\"id\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        key=True,\n",
    "        sortable=True,\n",
    "        filterable=True,\n",
    "        facetable=True,\n",
    "    ),\n",
    "    SimpleField(name=\"filepath\", type=SearchFieldDataType.String),\n",
    "    SimpleField(name=\"page\", type=SearchFieldDataType.String),\n",
    "    SimpleField(name=\"polygon\", type=SearchFieldDataType.Collection(SearchFieldDataType.Double)),\n",
    "    SearchableField(name=\"content\", type=SearchFieldDataType.String, searchable=True),\n",
    "    SearchField(\n",
    "        name=\"contentVector\",\n",
    "        type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "        searchable=True,\n",
    "        vector_search_dimensions=azure_openai_embedding_dimensions,\n",
    "        vector_search_profile_name=\"myHnswProfile\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Configure the vector search configuration\n",
    "vector_search = VectorSearch(\n",
    "    algorithms=[HnswAlgorithmConfiguration(name=\"myHnsw\")],\n",
    "    profiles=[\n",
    "        VectorSearchProfile(\n",
    "            name=\"myHnswProfile\",\n",
    "            algorithm_configuration_name=\"myHnsw\",\n",
    "            vectorizer_name=\"myVectorizer\",\n",
    "        )\n",
    "    ],\n",
    "    vectorizers=[\n",
    "        AzureOpenAIVectorizer(\n",
    "            vectorizer_name=\"myVectorizer\",\n",
    "            parameters=AzureOpenAIVectorizerParameters(\n",
    "                resource_url=azure_openai_endpoint,\n",
    "                deployment_name=azure_openai_embedding_deployment,\n",
    "                model_name=embedding_model_name,\n",
    "                api_key=azure_openai_key,\n",
    "            ),\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "semantic_config = SemanticConfiguration(\n",
    "    name=\"my-semantic-config\",\n",
    "    prioritized_fields=SemanticPrioritizedFields(\n",
    "        content_fields=[SemanticField(field_name=\"content\")],\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Create the semantic settings with the configuration\n",
    "semantic_search = SemanticSearch(configurations=[semantic_config])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the search index with the semantic settings\n",
    "index = SearchIndex(\n",
    "    name=INDEX_NAME,\n",
    "    fields=fields,\n",
    "    vector_search=vector_search,\n",
    "    semantic_search=semantic_search,\n",
    ")\n",
    "result = index_client.create_or_update_index(index)\n",
    "print(f\"{result.name} created or updated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.documentintelligence.models import DocumentTable, DocumentParagraph\n",
    "from datetime import datetime\n",
    "\n",
    "def get_element(doc, pointer):\n",
    "    content_type, index = pointer.split('/')[1:]\n",
    "    if content_type == 'sections':\n",
    "        return\n",
    "    element = getattr(doc, content_type)[int(index)]\n",
    "    if hasattr(element, 'elements'):\n",
    "        for p in element.elements:\n",
    "            get_element(doc, p)\n",
    "    else:\n",
    "        if isinstance(element, DocumentTable):\n",
    "            last_row = 0\n",
    "            for item in element.cells:\n",
    "                if item.row_index > last_row:\n",
    "                    print()\n",
    "                    last_row = item.row_index\n",
    "                print(f\"{item.content:<80}\", end=\"\")\n",
    "        elif isinstance(element, DocumentParagraph):\n",
    "            print(f\"{'':>10} {element.content}\")\n",
    "\n",
    "def generate_embedding(text):\n",
    "    return (\n",
    "        aoai_client.embeddings.create(\n",
    "            input=[text],\n",
    "            model=azure_openai_embedding_deployment,\n",
    "            dimensions=azure_openai_embedding_dimensions,\n",
    "        ).data[0].embedding\n",
    "    )\n",
    "\n",
    "def chunk_document_whole(document):\n",
    "    id = 0\n",
    "    yield {\n",
    "        'id': id,\n",
    "        'page': \"1\",\n",
    "        'content': document.content,\n",
    "    }\n",
    "\n",
    "def chunk_document_by_page(document):\n",
    "    for i, page in enumerate(document.pages):\n",
    "        yield {\n",
    "            'id': i,\n",
    "            'page': str(i + 1),\n",
    "            'content': \"\\n\".join([line.content for line in page.lines]),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for pdf_name, pdf_doc in parsed_pdfs.items():\n",
    "    # Index chunks\n",
    "    for chunk in chunk_document_by_page(pdf_doc):\n",
    "        embedding = generate_embedding(chunk['content'])\n",
    "        doc = {\n",
    "            \"id\": f\"{pdf_name}_{chunk['id']}\",\n",
    "            \"filepath\": pdf_name,\n",
    "            \"page\": chunk['page'],\n",
    "            \"polygon\": [],\n",
    "            \"content\": chunk['content'],\n",
    "            \"contentVector\": embedding,\n",
    "        }\n",
    "        documents.append(doc)\n",
    "    \n",
    "print(f\"Indexing {len(documents)} document chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_client.upload_documents(documents=documents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
